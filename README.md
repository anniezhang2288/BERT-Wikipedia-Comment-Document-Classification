# BERT-Wikipedia-Comment-Document-Classification

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)
![Keras](https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&logo=Keras&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)
![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)


## Overview
This Python notebook is a sophisticated implementation of the BERT (Bidirectional Encoder Representations from Transformers) model, using TensorFlow, PyTorch, Keras, and the Hugging Face library. It's designed for the nuanced task of syntactic analysis of Wikipedia comments, utilizing the Corpus of Linguistic Acceptability (CoLA) dataset. The project demonstrates advanced NLP techniques by fine-tuning BERT with the BertForSequenceClassification class, achieving an impressive Matthews Correlation Coefficient (MCC) of 0.540.

## Features
- **Advanced NLP Modeling**: Utilizes BERT for deep syntactic understanding.
- **Fine-Tuning**: Employs BertForSequenceClassification for precise model adaptation.
- **High Performance**: Achieves a notable MCC of 0.540, indicating strong model accuracy.
- **GPU Acceleration**: Leverages GPU for efficient training and evaluation.

## Technical Implementation
- **TensorFlow & PyTorch**: For robust machine learning model development.
- **Keras**: Simplifies the API for model training and evaluation.
- **Hugging Face Library**: Provides the pre-trained BERT model and utilities.

## Usage
1. **Setup**: Ensure Python, TensorFlow, PyTorch, Keras, and Hugging Face are installed.
2. **Data Preparation**: Load the CoLA dataset and preprocess it for BERT.
3. **Model Training**: Fine-tune the BertForSequenceClassification model on the dataset.
4. **Evaluation**: Assess the model's performance using the MCC metric.

## Contributions
Contributions to this project are welcome. Please submit a pull request or issue to propose changes or additions.

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE.md) file for details.

---

For more information or inquiries, please contact [anniezhang2288@berkeley.edu](mailto:anniezhang2288@berkeley.edu).
